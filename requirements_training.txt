# Requirements for LoRA fine-tuning FLUX.1-schnell
torch>=2.0.0
torchvision>=0.15.0
diffusers>=0.24.0
transformers>=4.25.0
accelerate>=0.20.0
peft>=0.6.0
datasets>=2.0.0
Pillow>=9.0.0
numpy>=1.21.0
tqdm>=4.64.0
safetensors>=0.3.0
huggingface_hub>=0.16.0
xformers>=0.0.20  # Optional, for memory efficiency
bitsandbytes>=0.41.0  # Optional, for quantization
