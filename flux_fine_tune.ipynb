{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2096776",
   "metadata": {},
   "source": [
    "# Fine-tuning Flux Model\n",
    "This notebook demonstrates how to fine-tune the Flux diffusion model using Hugging Face Diffusers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install diffusers[training] accelerate transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, folder, tokenizer, transforms=None):\n",
    "        self.folder = folder\n",
    "        self.image_paths = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('png','jpg','jpeg'))]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms or T.Compose([\n",
    "            T.Resize((512,512)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        text = \"pixel art sprite\"  # Change to appropriate prompt or caption\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.tokenizer.model_max_length, return_tensors='pt')\n",
    "        return {\"pixel_values\": image, \"input_ids\": inputs.input_ids.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708503b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "model_id = \"your-flux-model-id\"  # e.g., \"sprited/flux-1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n",
    "scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "dataset = ImageTextDataset(\"path/to/your/images\", tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        text_embeddings = text_encoder(input_ids)[0]\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 10 == 0:\n",
    "            print(f\"  Step {step}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed515dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./flux-finetuned\"\n",
    "unet.save_pretrained(f\"{output_dir}/unet\")\n",
    "scheduler.save_pretrained(f\"{output_dir}/scheduler\")\n",
    "vae.save_pretrained(f\"{output_dir}/vae\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n",
    "text_encoder.save_pretrained(f\"{output_dir}/text_encoder\")\n",
    "print(\"Model saved to\", output_dir)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
